{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Add parts-of-speech bigrams to the bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_bigram = list(nltk.bigrams(pos))\n",
    "pattern = [('JJ', 'NN'),('JJ','NNS'),('NN','NNS')]\n",
    "import copy\n",
    "corpus_words_bi = copy.deepcopy(corpus_words)\n",
    "\n",
    "for i in range(len(pos_bigram)):\n",
    "    (a,b),(c,d) = pos_bigram[i]\n",
    "    if (b,d) in pattern:\n",
    "        corpus_words_bi.append(' '.join(corpus_words[i:i+len(pattern)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Define the function to create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_features_bigram(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for wd in corpus_words_bi:\n",
    "        features['contains({})'.format(wd)] = (wd in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Find out the unigrams and bigrams in each job description that match the part-of-speech patterns we defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(len(fullset)):\n",
    "    words = re.sub(r'[^\\w\\s]','',fullset.FullDescription[i])\n",
    "    tokens = nltk.word_tokenize(words.lower().decode('utf-8'))\n",
    "    tokens = [w for w in tokens if w.isalpha()==True]\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    bigram = list(nltk.bigrams(tags))\n",
    "    for n in range(len(bigram)):\n",
    "        (a,b),(c,d) = bigram[n]\n",
    "        if (b,d) in pattern:\n",
    "            tokens.append(' '.join(tokens[n:n+len(pattern)]))\n",
    "    feature = document_features_bigram(tokens)\n",
    "    fullset.token[i] = feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Run classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.808"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullset['feature_sets'] = zip(fullset.token, fullset.category)\n",
    "featuresets = fullset.feature_sets.tolist()\n",
    "train_set, test_set = featuresets[:500], featuresets[500:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My version of B2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Lemmatize words according to their pos in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(fullset)):\n",
    "    words = re.sub(r'[^\\w\\s]','',fullset.FullDescription[i])\n",
    "    token = nltk.word_tokenize(words.lower().decode('utf-8'))\n",
    "    pos_i = nltk.pos_tag(token)\n",
    "    wordnet_tag ={'NN':'n','JJ':'a','VB':'v','RB':'r'}\n",
    "    resultList = []\n",
    "    for t in pos_i:\n",
    "        try: resultList.append(wnl.lemmatize(t[0],wordnet_tag[t[1][:2]]))\n",
    "        except: resultList.append(wnl.lemmatize(t[0]))\n",
    "    feature = document_features(resultList)\n",
    "    fullset.token[i] = feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Run classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fullset['feature_sets'] = zip(fullset.token, fullset.category)\n",
    "featuresets = fullset.feature_sets.tolist()\n",
    "train_set, test_set = featuresets[:500], featuresets[500:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.818"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
